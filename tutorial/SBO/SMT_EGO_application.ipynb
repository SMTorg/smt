{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><a href=\"https://colab.research.google.com/github/SMTorg/smt/blob/master/tutorial/SBO/SMT_EGO_application.ipynb\" target=\"_blank\" rel=\"nofollow\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"jumbotron text-left\"><b>\n",
    "    \n",
    "This tutorial describes how to use the SMT toolbox to do some Bayesian Optimization (EGO method) to solve unconstrained optimization problem\n",
    "<div>\n",
    "    \n",
    "Rémy Priem, Nathalie Bartoli, Paul Saves, Heine Røstum - December 2024\n",
    "\n",
    "based on `SMT 2.8.1 version` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"alert alert-success\" style=\"padding:1em\">\n",
    "To use SMT models, please follow this link : https://github.com/SMTorg/SMT/blob/master/README.md. The documentation is available here: http://smt.readthedocs.io/en/latest/\n",
    "</p>\n",
    "\n",
    "The reference paper is available \n",
    "here https://www.sciencedirect.com/science/article/pii/S0965997818309360?via%3Dihub \n",
    "\n",
    "or as a preprint: http://mdolab.engin.umich.edu/content/python-surrogate-modeling-framework-derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info fade in\" id=\"d110\">\n",
    "<p>In this notebook, two examples are presented to illustrate Bayesian Optimization</p>\n",
    "<ol> - a 1D-example (xsinx function) where the algorithm is explicitely given and the use of different criteria is presented </ol>\n",
    "<ol> - a 2D-exemple (Rosenbrock function) where the EGO algorithm from SMT is used  </ol>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info fade in\" id=\"d110\">\n",
    "<p>Latest updates</p>\n",
    "<ol> - add tunneling penalization to improve EGO convergence  </ol>\n",
    "<ol> - add qEI  to add several interesting points in parallel  </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid warning messages\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "def fun_1d(point):\n",
    "    return np.atleast_2d((point - 3.5) * np.sin((point - 3.5) / (np.pi)))\n",
    "\n",
    "\n",
    "X_plot = np.atleast_2d(np.linspace(0, 25, 10000)).T\n",
    "Y_plot = fun_1d(X_plot)\n",
    "\n",
    "\n",
    "lines = []\n",
    "fig = plt.figure(figsize=[5, 5])\n",
    "ax = fig.add_subplot(111)\n",
    "(true_fun,) = ax.plot(X_plot, Y_plot)\n",
    "lines.append(true_fun)\n",
    "ax.set_title(r\"$x \\sin{x}$ function\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "plt.show()\n",
    "\n",
    "# dimension of the problem\n",
    "ndim = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the training data are the points of the design of experiments=[0,7,25]. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdoe = np.atleast_2d([0, 7, 25]).T\n",
    "ydoe = fun_1d(xdoe)\n",
    "n_doe = xdoe.shape[0]\n",
    "print(\"Number of DOE points = \", n_doe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the GP model with a square exponential kernel with SMT toolbox knowing $(x_{data}, y_{data})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from smt.surrogate_models import KRG\n",
    "\n",
    "########### The Kriging model\n",
    "\n",
    "# The variable 'theta0' is a list of length ndim.\n",
    "t = KRG(theta0=[1e-2] * ndim, print_prediction=False, corr=\"squar_exp\")\n",
    "\n",
    "# Training\n",
    "t.set_training_values(xdoe, ydoe)\n",
    "t.train()\n",
    "\n",
    "\n",
    "# Prediction of the  points for the plot\n",
    "Y_GP_plot = t.predict_values(X_plot)\n",
    "Y_GP_plot_var = t.predict_variances(X_plot)\n",
    "fig = plt.figure(figsize=[5, 5])\n",
    "ax = fig.add_subplot(111)\n",
    "(true_fun,) = ax.plot(X_plot, Y_plot)\n",
    "(doe,) = ax.plot(xdoe, ydoe, linestyle=\"\", marker=\"s\", markersize=14, color=\"blue\")\n",
    "(gp,) = ax.plot(X_plot, Y_GP_plot, linestyle=\"--\", color=\"g\")\n",
    "sig_plus = Y_GP_plot + 3 * np.sqrt(Y_GP_plot_var)\n",
    "sig_moins = Y_GP_plot - 3 * np.sqrt(Y_GP_plot_var)\n",
    "un_gp = ax.fill_between(\n",
    "    X_plot.T[0], sig_plus.T[0], sig_moins.T[0], alpha=0.3, color=\"g\"\n",
    ")\n",
    "lines = [true_fun, doe, gp, un_gp]\n",
    "ax.set_title(r\"$x \\sin{x}$ function\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.legend(lines, [\"True function\", \"DOE\", \"GPR prediction\", \"99 % confidence\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Bayesian optimization is defined by Jonas Mockus in (Mockus, 1975) as an optimization technique based upon the minimization of the expected deviation from the extremum of the studied function. \n",
    "\n",
    "The objective function is treated as a black-box function. A Bayesian strategy sees the objective as a random function and places a prior over it. The prior captures our beliefs about the behavior of the function. After gathering the function evaluations, which are treated as data, the prior is updated to form the posterior distribution over the objective function. The posterior distribution, in turn, is used to construct an acquisition function (often also referred to as infill sampling criterion) that determines what the next query point should be.\n",
    "\n",
    "One of the earliest bodies of work on Bayesian optimisation that we are aware of is (Kushner, 1962 ; Kushner, 1964). Kushner used Wiener processes for one-dimensional problems. Kushner’s decision model was based on maximizing the probability of improvement, and included a parameter that controlled the trade-off between ‘more global’ and ‘more local’ optimization, in the same spirit as the Exploration/Exploitation trade-off.\n",
    "\n",
    "Meanwhile, in the former Soviet Union, Mockus and colleagues developed a multidimensional Bayesian optimization method using linear combinations of Wiener fields, some of which was published in English in (Mockus, 1975). This paper also describes an acquisition function that is based on myopic expected improvement of the posterior, which has been widely adopted in Bayesian optimization as the Expected Improvement function.\n",
    "\n",
    "In 1998, Jones used Gaussian processes together with the expected improvement function to successfully perform derivative-free optimization and experimental design through an algorithm called  Efficient  Global  Optimization, or EGO (Jones, 1998).\n",
    "\n",
    "## Efficient Global Optimization\n",
    "\n",
    "In what follows, we describe the Efficient Global Optimization (EGO) algorithm, as published in (Jones, 1998).\n",
    "\n",
    "Let $F$ be an expensive black-box function to be minimized. We sample $F$ at the different locations  $X = \\{x_1, x_2,\\ldots,x_n\\}$ yielding the responses $Y = \\{y_1, y_2,\\ldots,y_n\\}$. We build a Kriging model (also called Gaussian process) with a mean function $\\mu$ and a variance function $\\sigma^{2}$.\n",
    "\n",
    "The next step is to compute the criterion EI. To do this, let us denote:\n",
    "$$f_{min} = \\min \\{y_1, y_2,\\ldots,y_n\\}.$$\n",
    "The Expected Improvement funtion (EI) can be expressed:\n",
    "$$E[I(x)] = E[\\max(f_{min}-Y, 0)],$$\n",
    "where $Y$ is the random variable following the distribution $\\mathcal{N}(\\mu(x), \\sigma^{2}(x))$.\n",
    "By expressing the right-hand side of EI expression as an integral, and applying some tedious integration by parts, one can express the expected improvement in closed form: \n",
    "$$\n",
    "E[I(x)] = (f_{min} - \\mu(x))\\Phi\\left(\\frac{f_{min} - \\mu(x)}{\\sigma(x)}\\right) + \\sigma(x) \\phi\\left(\\frac{f_{min} - \\mu(x)}{\\sigma(x)}\\right)\n",
    "$$\n",
    "where $\\Phi(\\cdot)$ and $\\phi(\\cdot)$ are respectively the cumulative and probability density functions of $\\mathcal{N}(0,1)$.\n",
    "\n",
    "Next, we determine our next sampling point as :\n",
    "\\begin{align}\n",
    "x_{n+1} = \\arg \\max_{x} \\left(E[I(x)]\\right)\n",
    "\\end{align}\n",
    "\n",
    "We then test the response $y_{n+1}$ of our black-box function $F$ at $x_{n+1}$, rebuild the model taking into account the new information gained, and research the point of maximum expected improvement again.\n",
    "\n",
    "We summarize here the EGO algorithm:\n",
    "\n",
    "EGO(F, $n_{iter}$) \\# Find the best minimum of $\\operatorname{F}$ in $n_{iter}$ iterations  \n",
    "For ($i=0:n_{iter}$)  \n",
    "\n",
    "* $mod = {model}(X, Y)$  \\# surrogate model based on sample vectors $X$ and $Y$  \n",
    "* $f_{min} = \\min Y$  \n",
    "* $x_{i+1} = \\arg \\max {EI}(mod, f_{min})$ \\# choose $x$ that maximizes EI  \n",
    "* $y_{i+1} = {F}(x_{i+1})$ \\# Probe the function at most promising point $x_{i+1}$  \n",
    "* $X = [X,x_{i+1}]$  \n",
    "* $Y = [Y,y_{i+1}]$   \n",
    "* $i = i+1$  \n",
    "\n",
    "$f_{min} = \\min Y$  \n",
    "Return : $f_{min}$ \\# This is the best known solution after $n_{iter}$ iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to optimize this function by using Bayesian Optimization and comparing\n",
    "- Surrogate Based optimization (SBO)\n",
    "- Expected Improvement criterion (EI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first step we compute the EI criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def EI(GP, points, f_min):\n",
    "    pred = GP.predict_values(points)\n",
    "    var = GP.predict_variances(points)\n",
    "    args0 = (f_min - pred) / np.sqrt(var)\n",
    "    args1 = (f_min - pred) * norm.cdf(args0)\n",
    "    args2 = np.sqrt(var) * norm.pdf(args0)\n",
    "\n",
    "    if var.size == 1 and var == 0.0:  # can be use only if one point is computed\n",
    "        return 0.0\n",
    "\n",
    "    ei = args1 + args2\n",
    "    return ei\n",
    "\n",
    "\n",
    "Y_GP_plot = t.predict_values(X_plot)\n",
    "Y_GP_plot_var = t.predict_variances(X_plot)\n",
    "Y_EI_plot = EI(t, X_plot, np.min(ydoe))\n",
    "\n",
    "fig = plt.figure(figsize=[10, 10])\n",
    "ax = fig.add_subplot(111)\n",
    "(true_fun,) = ax.plot(X_plot, Y_plot)\n",
    "(doe,) = ax.plot(xdoe, ydoe, linestyle=\"\", marker=\"s\", markersize=14, color=\"blue\")\n",
    "(gp,) = ax.plot(X_plot, Y_GP_plot, linestyle=\"--\", color=\"g\")\n",
    "sig_plus = Y_GP_plot + 3 * np.sqrt(Y_GP_plot_var)\n",
    "sig_moins = Y_GP_plot - 3 * np.sqrt(Y_GP_plot_var)\n",
    "un_gp = ax.fill_between(\n",
    "    X_plot.T[0], sig_plus.T[0], sig_moins.T[0], alpha=0.3, color=\"g\"\n",
    ")\n",
    "ax1 = ax.twinx()\n",
    "(ei,) = ax1.plot(X_plot, Y_EI_plot, color=\"red\")\n",
    "lines = [true_fun, doe, gp, un_gp, ei]\n",
    "ax.set_title(r\"$x \\sin{x}$ function\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax1.set_ylabel(\"ei\")\n",
    "fig.legend(\n",
    "    lines,\n",
    "    [\n",
    "        \"True function\",\n",
    "        \"DOE\",\n",
    "        \"GPR prediction\",\n",
    "        \"99 % confidence\",\n",
    "        \"Expected Improvement\",\n",
    "    ],\n",
    "    loc=[0.13, 0.64],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the EGO method and compare it to other infill criteria \n",
    "- SBO (surrogate based optimization): directly using the prediction of the surrogate model ($\\mu$)\n",
    "- LCB (Lower Confidence bound): using the confidence interval : $\\mu -3 \\times \\sigma$\n",
    "- EI for expected Improvement (EGO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surrogate Based optimization: min the Surrogate model by using the mean mu\n",
    "def SBO(GP, point):\n",
    "    res = GP.predict_values(point)\n",
    "    return res\n",
    "\n",
    "\n",
    "# lower confidence bound optimization: minimize by using mu - 3*sigma\n",
    "def LCB(GP, point):\n",
    "    pred = GP.predict_values(point)\n",
    "    var = GP.predict_variances(point)\n",
    "    res = pred - 3.0 * np.sqrt(var)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IC = \"EI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "plt.ioff()\n",
    "\n",
    "\n",
    "n_iter = 15\n",
    "\n",
    "gpr = KRG(theta0=[1e-2] * ndim, print_global=False)\n",
    "\n",
    "# initialization\n",
    "x_data = xdoe\n",
    "y_data = ydoe\n",
    "\n",
    "for k in range(n_iter):\n",
    "    x_start = np.atleast_2d(np.random.rand(20) * 25).T\n",
    "    f_min_k = np.min(y_data)\n",
    "    gpr.set_training_values(xdoe, ydoe)\n",
    "    gpr.train()\n",
    "    if IC == \"EI\":\n",
    "\n",
    "        def obj_k(x):\n",
    "            return -EI(gpr, np.atleast_2d(x), f_min_k)\n",
    "    elif IC == \"SBO\":\n",
    "\n",
    "        def obj_k(x):\n",
    "            return SBO(gpr, np.atleast_2d(x))\n",
    "    elif IC == \"LCB\":\n",
    "\n",
    "        def obj_k(x):\n",
    "            return LCB(gpr, np.atleast_2d(x))\n",
    "\n",
    "    opt_all = np.array(\n",
    "        [\n",
    "            minimize(lambda x: float(obj_k(x)), x_st, method=\"SLSQP\", bounds=[(0, 25)])\n",
    "            for x_st in x_start\n",
    "        ]\n",
    "    )\n",
    "    opt_success = opt_all[[opt_i[\"success\"] for opt_i in opt_all]]\n",
    "    obj_success = np.array([opt_i[\"fun\"] for opt_i in opt_success])\n",
    "    ind_min = np.argmin(obj_success)\n",
    "    opt = opt_success[ind_min]\n",
    "    x_et_k = opt[\"x\"]\n",
    "\n",
    "    y_et_k = fun_1d(x_et_k)\n",
    "\n",
    "    y_data = np.atleast_2d(np.append(y_data, y_et_k)).T\n",
    "    x_data = np.atleast_2d(np.append(x_data, x_et_k)).T\n",
    "\n",
    "    Y_GP_plot = gpr.predict_values(X_plot)\n",
    "    Y_GP_plot_var = gpr.predict_variances(X_plot)\n",
    "    Y_EI_plot = -EI(gpr, X_plot, f_min_k)\n",
    "\n",
    "    fig = plt.figure(figsize=[10, 10])\n",
    "    ax = fig.add_subplot(111)\n",
    "    if IC == \"LCB\" or IC == \"SBO\":\n",
    "        (ei,) = ax.plot(X_plot, Y_EI_plot, color=\"red\")\n",
    "    else:\n",
    "        ax1 = ax.twinx()\n",
    "        (ei,) = ax1.plot(X_plot, Y_EI_plot, color=\"red\")\n",
    "    (true_fun,) = ax.plot(X_plot, Y_plot)\n",
    "    (doe,) = ax.plot(xdoe, ydoe, linestyle=\"\", marker=\"s\", markersize=14, color=\"blue\")\n",
    "    (data,) = ax.plot(\n",
    "        x_data[0 : k + n_doe],\n",
    "        y_data[0 : k + n_doe],\n",
    "        linestyle=\"\",\n",
    "        marker=\"o\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    "    (opt,) = ax.plot(\n",
    "        x_data[k + n_doe], y_data[k + n_doe], linestyle=\"\", marker=\"*\", color=\"r\"\n",
    "    )\n",
    "    (gp,) = ax.plot(X_plot, Y_GP_plot, linestyle=\"--\", color=\"g\")\n",
    "    sig_plus = Y_GP_plot + 3 * np.sqrt(Y_GP_plot_var)\n",
    "    sig_moins = Y_GP_plot - 3 * np.sqrt(Y_GP_plot_var)\n",
    "    un_gp = ax.fill_between(\n",
    "        X_plot.T[0], sig_plus.T[0], sig_moins.T[0], alpha=0.3, color=\"g\"\n",
    "    )\n",
    "    lines = [true_fun, doe, data, gp, un_gp, opt, ei]\n",
    "    ax.set_title(r\"$x \\sin{x}$ function\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.legend(\n",
    "        lines,\n",
    "        [\n",
    "            \"True function\",\n",
    "            \"initial DOE\",\n",
    "            \"Data\",\n",
    "            \"GPR prediction\",\n",
    "            \"99 % confidence\",\n",
    "            \"Next point to Evaluate\",\n",
    "            \"Infill Criteria\",\n",
    "        ],\n",
    "    )\n",
    "    plt.savefig(\"Optimisation %d\" % k)\n",
    "    plt.close(fig)\n",
    "\n",
    "ind_best = np.argmin(y_data)\n",
    "x_opt = x_data[ind_best]\n",
    "y_opt = y_data[ind_best]\n",
    "\n",
    "print(\"Results : X = %s, Y = %s\" % (x_opt, y_opt))\n",
    "\n",
    "fig = plt.figure(figsize=[10, 10])\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.axes.get_xaxis().set_visible(False)\n",
    "ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "ims = []\n",
    "for k in range(n_iter):\n",
    "    image_pt = mpimg.imread(\"Optimisation %d.png\" % k)\n",
    "    im = plt.imshow(image_pt)\n",
    "    ims.append([im])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=500)\n",
    "HTML(ani.to_jshtml())\n",
    "\n",
    "\n",
    "# Check the optimal point is x_opt=18.9, y_opt =-15.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Use the EGO from SMT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smt.applications.ego import EGO\n",
    "from smt.sampling_methods import LHS\n",
    "from smt.design_space import (\n",
    "    DesignSpace,\n",
    "    IntegerVariable,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Choose your criterion to perform the optimization: EI, SBO or LCB\n",
    "* Choose the size of the initial DOE\n",
    "* Choose the number of EGO iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 6\n",
    "xlimits = np.array([[0.0, 25.0]])\n",
    "\n",
    "random_state = 42  # for reproducibility\n",
    "design_space = DesignSpace(xlimits, random_state=random_state)\n",
    "\n",
    "\n",
    "criterion = \"EI\"  #'EI' or 'SBO' or 'LCB'\n",
    "\n",
    "ego = EGO(\n",
    "    n_iter=n_iter,\n",
    "    criterion=criterion,\n",
    "    xdoe=xdoe,\n",
    "    surrogate=KRG(design_space=design_space, print_global=False),\n",
    "    random_state=random_state,\n",
    ")\n",
    "\n",
    "x_opt, y_opt, _, x_data, y_data = ego.optimize(fun=fun_1d)\n",
    "print(\"Minimum in x={:.1f} with f(x)={:.1f}\".format(x_opt.item(), y_opt.item()))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=[10, 10])\n",
    "for i in range(n_iter):\n",
    "    k = n_doe + i\n",
    "    x_data_k = x_data[0:k]\n",
    "    y_data_k = y_data[0:k]\n",
    "    ego.gpr.set_training_values(x_data_k, y_data_k)\n",
    "    ego.gpr.train()\n",
    "\n",
    "    y_gp_plot = ego.gpr.predict_values(X_plot)\n",
    "    y_gp_plot_var = ego.gpr.predict_variances(X_plot)\n",
    "    y_ei_plot = -ego.EI(X_plot)\n",
    "\n",
    "    ax = fig.add_subplot((n_iter + 1) // 2, 2, i + 1)\n",
    "    ax1 = ax.twinx()\n",
    "    (ei,) = ax1.plot(X_plot, y_ei_plot, color=\"red\")\n",
    "\n",
    "    (true_fun,) = ax.plot(X_plot, Y_plot)\n",
    "    # (doe,) = ax.plot(\n",
    "    # xdoe, ydoe, linestyle=\"\", marker=\"s\", markersize=14, color=\"blue\"\n",
    "    # )\n",
    "    (doe,) = ax.plot(xdoe, ydoe, linestyle=\"\", marker=\"s\", markersize=14, color=\"blue\")\n",
    "    (data,) = ax.plot(x_data_k, y_data_k, linestyle=\"\", marker=\"o\", color=\"orange\")\n",
    "    if i < n_iter - 1:\n",
    "        (opt,) = ax.plot(x_data[k], y_data[k], linestyle=\"\", marker=\"*\", color=\"r\")\n",
    "    (gp,) = ax.plot(X_plot, y_gp_plot, linestyle=\"--\", color=\"g\")\n",
    "    sig_plus = y_gp_plot + 3 * np.sqrt(y_gp_plot_var)\n",
    "    sig_moins = y_gp_plot - 3 * np.sqrt(y_gp_plot_var)\n",
    "    un_gp = ax.fill_between(\n",
    "        X_plot.T[0], sig_plus.T[0], sig_moins.T[0], alpha=0.3, color=\"g\"\n",
    "    )\n",
    "    lines = [true_fun, data, gp, un_gp, opt, ei]\n",
    "    fig.suptitle(\"EGO optimization of $f(x) = x \\\\sin{x}$\")\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4, top=0.8)\n",
    "    ax.set_title(\"iteration {}\".format(i + 1))\n",
    "    fig.legend(\n",
    "        lines,\n",
    "        [\n",
    "            \"f(x)=xsin(x)\",\n",
    "            \"Given data points\",\n",
    "            \"Kriging prediction\",\n",
    "            \"Kriging 99% confidence interval\",\n",
    "            \"Next point to evaluate\",\n",
    "            \"Expected improvment function\",\n",
    "        ],\n",
    "    )\n",
    "plt.show()\n",
    "# Check the optimal point is x_opt=18.9, y_opt =-15.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with a 2D function : 2D Rosenbrock function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rosenbrock Function  in dimension N\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}) = \\sum_{i=1}^{N-1} 100 (x_{i+1} - x_i^2 )^2 + (1-x_i)^2 \\quad \\mbox{where} \\quad \\mathbf{x} = [x_1, \\ldots, x_N] \\in \\mathbb{R}^N.\n",
    "$$\n",
    "\n",
    "$$x_i \\in [-2,2]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the rosenbrock function\n",
    "def rosenbrock(x):\n",
    "    \"\"\"\n",
    "    Evaluate objective and constraints for the Rosenbrock test case:\n",
    "    \"\"\"\n",
    "    n, dim = x.shape\n",
    "\n",
    "    # parameters:\n",
    "    Opt = []\n",
    "    Opt_point_scalar = 1\n",
    "    # construction of O vector\n",
    "    for i in range(0, dim):\n",
    "        Opt.append(Opt_point_scalar)\n",
    "\n",
    "    # Construction of Z vector\n",
    "    Z = np.zeros((n, dim))\n",
    "    for i in range(0, dim):\n",
    "        Z[:, i] = x[:, i] - Opt[i] + 1\n",
    "\n",
    "    # Sum\n",
    "    sum1 = np.zeros((n, 1))\n",
    "    for i in range(0, dim - 1):\n",
    "        sum1[:, 0] += 100 * (((Z[:, i] ** 2) - Z[:, i + 1]) ** 2) + ((Z[:, i] - 1) ** 2)\n",
    "\n",
    "    return sum1\n",
    "\n",
    "\n",
    "xlimits = np.array([[-2, 2], [-2, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# To plot the Rosenbrock function\n",
    "num_plot = 50  # to plot rosenbrock\n",
    "x = np.linspace(xlimits[0][0], xlimits[0][1], num_plot)\n",
    "res = []\n",
    "for x0 in x:\n",
    "    for x1 in x:\n",
    "        res.append(rosenbrock(np.array([[x0, x1]])))\n",
    "res = np.array(res)\n",
    "res = res.reshape((50, 50)).T\n",
    "X, Y = np.meshgrid(x, x)\n",
    "fig = plt.figure(figsize=[10, 10])\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "surf = ax.plot_surface(\n",
    "    X, Y, res, cmap=cm.coolwarm, linewidth=0, antialiased=False, alpha=0.5\n",
    ")\n",
    "plt.title(\" Rosenbrock function\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = \"EI\"  #'EI' or 'SBO' or 'LCB'\n",
    "\n",
    "# number of points in the initial DOE\n",
    "ndoe = 10  # (at least ndim+1)\n",
    "\n",
    "# number of iterations with EGO\n",
    "n_iter = 30\n",
    "\n",
    "design_space = DesignSpace(xlimits)\n",
    "\n",
    "# Build the initial DOE, add the random_state option to have the reproducibility of the LHS points\n",
    "sampling = LHS(xlimits=xlimits, random_state=1)\n",
    "xdoe = sampling(ndoe)\n",
    "\n",
    "\n",
    "# EGO call\n",
    "sm = KRG(design_space=design_space, n_start=25, print_global=False)\n",
    "\n",
    "ego = EGO(\n",
    "    n_iter=n_iter,\n",
    "    criterion=criterion,\n",
    "    xdoe=xdoe,\n",
    "    surrogate=sm,\n",
    "    n_start=30,  # to do multistart for maximizing the acquisition function\n",
    ")\n",
    "\n",
    "\n",
    "x_opt, y_opt, ind_best, x_data, y_data = ego.optimize(fun=rosenbrock)\n",
    "\n",
    "print(\n",
    "    \"Xopt for Rosenbrock \", x_opt, y_opt, \" obtained using EGO criterion = \", criterion\n",
    ")\n",
    "print(\"Check if the optimal point is Xopt= (1,1) with the Y value=0\")\n",
    "print(\n",
    "    \"if not you can increase the number of iterations with n_iter but the CPU will increase also.\"\n",
    ")\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the Rosenbrock function\n",
    "# 3D plot\n",
    "x = np.linspace(xlimits[0][0], xlimits[0][1], num_plot)\n",
    "res = []\n",
    "for x0 in x:\n",
    "    for x1 in x:\n",
    "        res.append(rosenbrock(np.array([[x0, x1]])))\n",
    "res = np.array(res)\n",
    "res = res.reshape((50, 50)).T\n",
    "X, Y = np.meshgrid(x, x)\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "\n",
    "surf = ax.plot_surface(\n",
    "    X, Y, res, cmap=cm.coolwarm, linewidth=0, antialiased=False, alpha=0.5\n",
    ")\n",
    "# to add the points provided by EGO\n",
    "ax.scatter(\n",
    "    x_data[:ndoe, 0],\n",
    "    x_data[:ndoe, 1],\n",
    "    y_data[:ndoe],\n",
    "    zdir=\"z\",\n",
    "    marker=\".\",\n",
    "    c=\"k\",\n",
    "    s=100,\n",
    "    label=\"Initial DOE\",\n",
    ")\n",
    "ax.scatter(\n",
    "    x_data[ndoe:, 0],\n",
    "    x_data[ndoe:, 1],\n",
    "    y_data[ndoe:],\n",
    "    zdir=\"z\",\n",
    "    marker=\"x\",\n",
    "    c=\"r\",\n",
    "    s=100,\n",
    "    label=\"Added point\",\n",
    ")\n",
    "ax.scatter(\n",
    "    x_opt[0],\n",
    "    x_opt[1],\n",
    "    y_opt,\n",
    "    zdir=\"z\",\n",
    "    marker=\"*\",\n",
    "    c=\"g\",\n",
    "    s=100,\n",
    "    label=\"EGO optimal point\",\n",
    ")\n",
    "\n",
    "plt.title(\" Rosenbrock function during EGO algorithm\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 2D plot\n",
    "# to add the points provided by EGO\n",
    "plt.plot(x_data[:ndoe, 0], x_data[:ndoe, 1], \".\", label=\"Initial DOE\")\n",
    "plt.plot(x_data[ndoe:, 0], x_data[ndoe:, 1], \"x\", c=\"r\", label=\"Added point\")\n",
    "plt.plot(x_opt[:1], x_opt[1:], \"*\", c=\"g\", label=\"EGO optimal point\")\n",
    "plt.plot([1], [1], \"*\", c=\"m\", label=\"Optimal point\")\n",
    "\n",
    "plt.title(\" Rosenbrock function during EGO algorithm\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the results by using only the mean information provided by surrogate model approximation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = \"SBO\"  #'EI' or 'SBO' or 'LCB'\n",
    "\n",
    "# number of iterations with EGO\n",
    "n_iter = 30\n",
    "\n",
    "\n",
    "# EGO call with the same initial DOE as before\n",
    "ego = EGO(\n",
    "    n_iter=n_iter,\n",
    "    criterion=criterion,\n",
    "    xdoe=xdoe,\n",
    "    surrogate=sm,\n",
    "    n_start=30,  # to do multistart for maximizing the acquisition function\n",
    ")\n",
    "\n",
    "\n",
    "x_opt, y_opt, ind_best, x_data, y_data = ego.optimize(fun=rosenbrock)\n",
    "\n",
    "print(\n",
    "    \"Xopt for Rosenbrock \", x_opt, y_opt, \" obtained using EGO criterion = \", criterion\n",
    ")\n",
    "print(\"Check if the optimal point is Xopt=(1,1) with the Y value=0\")\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EGO and tunneling penalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of tunneling  is to penalize the acquisition function in the neighbourhood of already evaluated points. The associated reference is given by:\n",
    "\n",
    "Zhang, S., & Norato, J. A. (2018, August). Finding better local optima in topology optimization via tunneling. In International Design Engineering Technical Conferences and Computers and Information in Engineering Conference (Vol. 51760, p. V02BT03A014). American Society of Mechanical Engineers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smt.applications.mixed_integer import MixedIntegerSamplingMethod\n",
    "from smt.sampling_methods import FullFactorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the plot function for 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotEgo(criterion, Xsol, Ysol, xdoe, bounds, npt, n_iter, tunneling, sm):\n",
    "    ego = EGO(\n",
    "        n_iter=n_iter,\n",
    "        criterion=criterion,\n",
    "        xdoe=xdoe,\n",
    "        n_start=20,\n",
    "        n_max_optim=35,\n",
    "        enable_tunneling=tunneling,\n",
    "        surrogate=sm,\n",
    "    )\n",
    "    x_opt, y_opt, ind_best, x_data, y_data = ego.optimize(fun=fun)\n",
    "    print(\"Minimum in x={:.0f} with fun(x)={:.10f}\".format(int(x_opt), float(y_opt)))\n",
    "\n",
    "    x_plot = np.atleast_2d(np.linspace(bounds[0][0], bounds[0][1], 9 * (npt - 1) + 1)).T\n",
    "    fig = plt.figure(figsize=[15, 15])\n",
    "    n_doe = xdoe.size\n",
    "    for i in range(n_iter):\n",
    "        k = n_doe + i\n",
    "        x_data_k = x_data[0:k]\n",
    "        y_data_k = y_data[0:k]\n",
    "\n",
    "        # if check list, not already evaluated\n",
    "        y_data[k] = fun(x_data[k][:, np.newaxis])\n",
    "        ego.gpr.set_training_values(x_data_k, y_data_k)\n",
    "        ego.gpr.train()\n",
    "        y_gp_plot = ego.gpr.predict_values(x_plot)\n",
    "        y_gp_plot_var = ego.gpr.predict_variances(x_plot)\n",
    "        y_ei_plot = ego.EI(x_plot, False)\n",
    "        ax = fig.add_subplot((n_iter + 1) // 2, 2, i + 1)\n",
    "        ax1 = ax.twinx()\n",
    "        (ei,) = ax1.plot(x_plot, y_ei_plot, color=\"red\")\n",
    "\n",
    "        true_fun = ax.scatter(Xsol, Ysol, color=\"k\", marker=\"d\")\n",
    "        (data,) = ax.plot(x_data_k, y_data_k, linestyle=\"\", marker=\"o\", color=\"orange\")\n",
    "        if i < n_iter - 1:\n",
    "            (opt,) = ax.plot(x_data[k], y_data[k], linestyle=\"\", marker=\"*\", color=\"r\")\n",
    "            print(x_data[k], y_data[k])\n",
    "        (gp,) = ax.plot(x_plot, y_gp_plot, linestyle=\"--\", color=\"g\")\n",
    "        sig_plus = y_gp_plot + 3 * np.sqrt(y_gp_plot_var)\n",
    "        sig_moins = y_gp_plot - 3 * np.sqrt(y_gp_plot_var)\n",
    "        un_gp = ax.fill_between(\n",
    "            x_plot.T[0], sig_plus.T[0], sig_moins.T[0], alpha=0.3, color=\"g\"\n",
    "        )\n",
    "        lines = [true_fun, data, gp, un_gp, opt, ei]\n",
    "        fig.suptitle(\"EGO optimization of a set of points\")\n",
    "        fig.subplots_adjust(hspace=0.4, wspace=0.4, top=0.8)\n",
    "        ax.set_title(\"iteration {}\".format(i + 1))\n",
    "        fig.legend(\n",
    "            lines,\n",
    "            [\n",
    "                \"set of points\",\n",
    "                \"Given data points\",\n",
    "                \"Kriging prediction\",\n",
    "                \"Kriging 99% confidence interval\",\n",
    "                \"Next point to evaluate\",\n",
    "                \"Expected improvment function\",\n",
    "            ],\n",
    "        )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunneling for 1D function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(point):\n",
    "    return np.atleast_2d((point - 3.5) * np.sin((point - 3.5) / (np.pi)))\n",
    "\n",
    "\n",
    "bounds = np.array([[0, 25]])\n",
    "xlimits = bounds\n",
    "design_space = DesignSpace(\n",
    "    [\n",
    "        IntegerVariable(bounds[0][0], bounds[0][1]),  # to have integer variable\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = np.atleast_2d(np.arange(xlimits[0][0], xlimits[0][1] + 1, 1))\n",
    "Y_plot = fun(X_plot)\n",
    "npts = np.shape(X_plot)[1]\n",
    "plt.plot(X_plot, Y_plot, \"kd\")\n",
    "plt.title(\"Initial function with integer x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = \"EI\"  #'EI' or 'SBO' or 'LCB'\n",
    "\n",
    "sm = KRG(design_space=design_space, print_global=False)\n",
    "sampling = MixedIntegerSamplingMethod(FullFactorial, design_space)\n",
    "\n",
    "xdoe = sampling(4)\n",
    "n_iter = 3\n",
    "ego = EGO(\n",
    "    xdoe=xdoe,\n",
    "    n_iter=n_iter,\n",
    "    criterion=criterion,\n",
    "    surrogate=sm,\n",
    "    enable_tunneling=True,  # to impose tunneling\n",
    "    random_state=42,\n",
    "    n_start=30,\n",
    ")\n",
    "\n",
    "x_opt, y_opt, _, _, _ = ego.optimize(fun=fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Xopt for xsinx function, \",\n",
    "    x_opt,\n",
    "    y_opt,\n",
    "    \" obtained using EGO criterion = \",\n",
    "    criterion,\n",
    ")\n",
    "print(\"Check if the optimal point is 19 associated to y=-15.12\")\n",
    "print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_plot, Y_plot, \"kd\", \"reference\")\n",
    "plt.plot(xdoe, fun(xdoe), \"bo\", \"initial DOE\")\n",
    "plt.plot(x_opt, y_opt, \"r*\", \"Optimum found\")\n",
    "plt.title(\"Optimization done for the function with integer x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To have a nice plot with the different iterations\n",
    "tunneling = True\n",
    "n_iter = 5\n",
    "\n",
    "PlotEgo(criterion, X_plot, Y_plot, xdoe, bounds, npts, n_iter, tunneling, sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qEI example to add several interesting points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smt.applications.ego import Evaluator\n",
    "\n",
    "n_iter = 3\n",
    "n_parallel = 3\n",
    "n_start = 50\n",
    "xlimits = np.array([[0.0, 25.0]])\n",
    "xdoe = np.atleast_2d([0, 7, 25]).T\n",
    "n_doe = xdoe.size\n",
    "\n",
    "\n",
    "class ParallelEvaluator(Evaluator):\n",
    "    \"\"\"\n",
    "    Implement Evaluator interface using multiprocessing ThreadPool object (Python 3 only).\n",
    "    \"\"\"\n",
    "\n",
    "    def run(self, fun, x):\n",
    "        n_thread = 5\n",
    "        # Caveat: import are made here due to SMT documentation building process\n",
    "        import numpy as np\n",
    "        from sys import version_info\n",
    "        from multiprocessing.pool import ThreadPool\n",
    "\n",
    "        if version_info.major == 2:\n",
    "            return fun(x)\n",
    "        # Python 3 only\n",
    "        with ThreadPool(n_thread) as p:\n",
    "            return np.array(\n",
    "                [y[0] for y in p.map(fun, [np.atleast_2d(x[i]) for i in range(len(x))])]\n",
    "            )\n",
    "\n",
    "\n",
    "criterion = \"EI\"  #'EI' or 'SBO' or 'LCB'\n",
    "qEI = \"KBUB\"  # \"KB\", \"KBLB\", \"KBUB\", \"KBRand\"\n",
    "ego = EGO(\n",
    "    xdoe=xdoe,\n",
    "    n_iter=n_iter,\n",
    "    criterion=criterion,\n",
    "    surrogate=sm,\n",
    "    n_parallel=n_parallel,\n",
    "    qEI=qEI,\n",
    "    n_start=n_start,\n",
    "    evaluator=ParallelEvaluator(),\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "x_opt, y_opt, _, x_data, y_data = ego.optimize(fun=fun)\n",
    "print(\"Minimum in x={:.1f} with f(x)={:.1f}\".format(float(x_opt), float(y_opt)))\n",
    "\n",
    "x_plot = np.atleast_2d(np.linspace(0, 25, 100)).T\n",
    "y_plot = fun(x_plot)\n",
    "\n",
    "fig = plt.figure(figsize=[10, 10])\n",
    "for i in range(n_iter):\n",
    "    k = n_doe + (i) * (n_parallel)\n",
    "    x_data_k = x_data[0:k]\n",
    "    y_data_k = y_data[0:k]\n",
    "    x_data_sub = x_data_k.copy()\n",
    "    y_data_sub = y_data_k.copy()\n",
    "    for p in range(n_parallel):\n",
    "        ego.gpr.set_training_values(x_data_sub, y_data_sub)\n",
    "        ego.gpr.train()\n",
    "\n",
    "        y_ei_plot = -ego.EI(x_plot)\n",
    "        y_gp_plot = ego.gpr.predict_values(x_plot)\n",
    "        y_gp_plot_var = ego.gpr.predict_variances(x_plot)\n",
    "\n",
    "        x_data_sub = np.append(x_data_sub, x_data[k + p])\n",
    "        y_KB = ego._get_virtual_point(np.atleast_2d(x_data[k + p]), y_data_sub)\n",
    "\n",
    "        y_data_sub = np.append(y_data_sub, y_KB)\n",
    "\n",
    "        ax = fig.add_subplot(n_iter, n_parallel, i * (n_parallel) + p + 1)\n",
    "        ax1 = ax.twinx()\n",
    "        (ei,) = ax1.plot(x_plot, y_ei_plot, color=\"red\")\n",
    "\n",
    "        (true_fun,) = ax.plot(x_plot, y_plot)\n",
    "        (data,) = ax.plot(\n",
    "            x_data_sub[: -1 - p],\n",
    "            y_data_sub[: -1 - p],\n",
    "            linestyle=\"\",\n",
    "            marker=\"o\",\n",
    "            color=\"orange\",\n",
    "        )\n",
    "        (virt_data,) = ax.plot(\n",
    "            x_data_sub[-p - 1 : -1],\n",
    "            y_data_sub[-p - 1 : -1],\n",
    "            linestyle=\"\",\n",
    "            marker=\"o\",\n",
    "            color=\"g\",\n",
    "        )\n",
    "\n",
    "        (opt,) = ax.plot(\n",
    "            x_data_sub[-1], y_data_sub[-1], linestyle=\"\", marker=\"*\", color=\"r\"\n",
    "        )\n",
    "        (gp,) = ax.plot(x_plot, y_gp_plot, linestyle=\"--\", color=\"g\")\n",
    "        sig_plus = y_gp_plot + 3.0 * np.sqrt(y_gp_plot_var)\n",
    "        sig_moins = y_gp_plot - 3.0 * np.sqrt(y_gp_plot_var)\n",
    "        un_gp = ax.fill_between(\n",
    "            x_plot.T[0], sig_plus.T[0], sig_moins.T[0], alpha=0.3, color=\"g\"\n",
    "        )\n",
    "        lines = [true_fun, data, gp, un_gp, opt, ei, virt_data]\n",
    "        fig.suptitle(r\"EGOp optimization of $f(x) = x \\sin{x}$\")\n",
    "        fig.subplots_adjust(hspace=0.4, wspace=0.4, top=0.8)\n",
    "        ax.set_title(\"iteration {}.{}\".format(i, p))\n",
    "        fig.legend(\n",
    "            lines,\n",
    "            [\n",
    "                \"f(x)=xsin(x)\",\n",
    "                \"Given data points\",\n",
    "                \"Kriging prediction\",\n",
    "                \"Kriging 99% confidence interval\",\n",
    "                \"Next point to evaluate\",\n",
    "                \"Expected improvment function\",\n",
    "                \"Virtula data points\",\n",
    "            ],\n",
    "        )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" >\n",
    "If you use hierarchical variables and the size of your doe greater than 30 points, you may leverage the `numba` JIT compiler to speed up the computation\n",
    "To do so:\n",
    "    \n",
    " - install numba library\n",
    "    \n",
    "     `pip install numba`\n",
    "    \n",
    "    \n",
    " - and define the environment variable `USE_NUMBA_JIT = 1` (unset or 0 if you do not want to use numba) \n",
    "    \n",
    "     - Linux: export USE_NUMBA_JIT = 1\n",
    "    \n",
    "     - Windows: set USE_NUMBA_JIT = 1\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE: CONSTRAINED EXPECTED IMPROVEMENT (cEI)\n",
    "An example of a constrained optimization problem is shown, where the problem is solved using constrained Expected Imrovement (cEI). SMT is used for creating the initial sampling using Latin Hypercube Sampling (LHS). A typical constrained optimization problem can mathematically be formulated as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\textbf{x}^* = \\underset{\\textbf{x} \\in \\textbf{X}}{\\text{arg min}} &\\; f(\\textbf{x}) \\\\\n",
    "\\mathrm{s.t}\\quad& g_i(\\textbf{x}) \\leq \\lambda_i, \\quad i = 1, 2,...,I\n",
    "\\end{align}\n",
    "$$\n",
    "Where $\\textbf{x}$ is a d-dimensional vector of design variables, and the design space $\\textbf{X}$ is a bounded subset of $\\mathbb{R}^d$. $f: \\textbf{X} \\rightarrow \\mathbb{R}$ is the objective function, subjected to $I$ number of inequality constraints expressed by the $i^{th}$ constraint function as $g_i: \\textbf{X} \\rightarrow \\mathbb{R}$. The goal is to find the value of $\\textbf{x}$ that minimizes the evaluation of the objective function $f(\\textbf{x})$, and also fulfills the constraint evaluations, $g_i(\\textbf{x}) \\leq \\lambda_i$. This optimum combination of design variables is denoted $\\textbf{x}^*$.\n",
    "\n",
    "In this example, the objective function $f(\\textbf{x})$ is the 2D Branin function, and the constraint $g(\\textbf{x})$ is a Sphere function, which is to be less than or equal to 40, i.e. $\\lambda = 40$. The example can easily be expanded to include multiple constraints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following implements a penalization technique by Probability of Feasibility (PoF).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the sampling set using Latin Hypercube Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from smt.problems import Branin, Sphere\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "ndim = 2\n",
    "goal = Branin(ndim=ndim)\n",
    "constraint = Sphere(ndim=ndim)\n",
    "\n",
    "x0_lim = [-5, 10]\n",
    "x1_lim = [0, 15]\n",
    "\n",
    "xlimits = np.array([x0_lim, x1_lim])\n",
    "sampling = LHS(\n",
    "    xlimits=xlimits, random_state=12\n",
    ")  # Random state is the seed number, only used for reproducibility\n",
    "num = 15\n",
    "x_t = sampling(num)\n",
    "\n",
    "y_t = goal(x_t)\n",
    "g_t = constraint(x_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training surrogates for the objective and constraint functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smt.surrogate_models import KRG\n",
    "\n",
    "# Training the surrogate for the objective function\n",
    "y_hat = KRG(theta0=[1e-2])\n",
    "y_hat.set_training_values(x_t, y_t)\n",
    "y_hat.train()\n",
    "\n",
    "# Training the surrogate for the constraint function\n",
    "g_hat = KRG(theta0=[1e-2])\n",
    "g_hat.set_training_values(x_t, g_t)\n",
    "g_hat.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Create plotting data\n",
    "num = 500\n",
    "x = np.zeros([num, 2])\n",
    "x[:, 0] = np.linspace(-5.0, 10.0, num)\n",
    "x[:, 1] = np.linspace(0.0, 15.0, num)\n",
    "\n",
    "X, Y = np.meshgrid(x[:, 0], x[:, 1])  # Create a grid\n",
    "f = np.zeros([num, num])\n",
    "g = np.zeros([num, num])\n",
    "for i in range(num):\n",
    "    f[:, i] = y_hat.predict_values(np.array([X[:, i], Y[:, i]]).T).flatten()\n",
    "    g[:, i] = g_hat.predict_values(np.array([X[:, i], Y[:, i]]).T).flatten()\n",
    "\n",
    "\n",
    "clear_output()\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# Ploting objective function\n",
    "contour1 = axes[0].contourf(\n",
    "    X, Y, f, levels=40, cmap=\"RdBu_r\"\n",
    ")  # Filled contours in red and blue\n",
    "contour_lines = axes[0].contour(\n",
    "    X, Y, f, levels=40, colors=\"black\"\n",
    ")  # Contour lines in black\n",
    "fig.colorbar(contour1, ax=axes[0])\n",
    "axes[0].scatter(x_t[:, 0], x_t[:, 1], color=\"black\")\n",
    "axes[0].set_title(\"Predicted object function\", fontsize=16)\n",
    "axes[0].set_xlabel(\"x1\")\n",
    "axes[0].set_ylabel(\"x2\")\n",
    "\n",
    "# Ploting constraint function\n",
    "contour2 = axes[1].contourf(\n",
    "    X, Y, g, levels=40, cmap=\"RdBu_r\"\n",
    ")  # Filled contours in red and blue\n",
    "contour_lines = axes[1].contour(\n",
    "    X, Y, g, levels=40, colors=\"black\"\n",
    ")  # Contour lines in black\n",
    "fig.colorbar(contour2, ax=axes[1])\n",
    "axes[1].scatter(x_t[:, 0], x_t[:, 1], color=\"black\")\n",
    "axes[1].set_title(\"Predicted constraint function\", fontsize=16)\n",
    "axes[1].set_xlabel(\"x1\")\n",
    "axes[1].set_ylabel(\"x2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the expected improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constrained expected improvement $\\text{cEI}(\\textbf{x})$ is calculated as: $$\\text{cEI}(\\textbf{x}) = \\text{EI}(\\textbf{x})\\text{PoF}(\\textbf{x})$$\n",
    "Here the Expected Improvement $\\text{EI}(\\textbf{x})$ is calculated as; \n",
    "$$\\text{EI}(\\textbf{x}) = (\\text{y}^* - \\hat{\\mu}_y(\\textbf{x}))\\Phi\\left(\\frac{\\text{y}^* - \\hat{\\mu}_y(\\textbf{x})}{\\hat{\\sigma}_y(\\textbf{x})}\\right)+\\hat{\\sigma}_y(\\textbf{x})\\phi\\left(\\frac{\\text{y}^* - \\hat{\\mu}_y(\\textbf{x})}{\\hat{\\sigma}_y(\\textbf{x})}\\right)$$\n",
    "Where $\\text{y}^*$ is the current best sampled value of the objective function that satisfies the constraints, $\\Phi$ and $\\phi$ are respectively the cumulative and probability density function of a standardized normal distribution, $\\hat{\\mu}_y$ and $\\hat{\\sigma}_y$ are respectively the predictive mean and variance of the objective function. The Probability of Feasability $\\text{PoF}(\\textbf{x})$ is calculated as;\n",
    "$$\\text{PoF}(\\textbf{x}) = \\prod_{i = 1}^{n_c}P(\\hat{g}_i(\\textbf{x}) \\leq \\lambda_i) = \\prod_{i = 1}^{n_c}\\Phi\\left(\\frac{\\lambda_i - \\hat{\\mu}_{gi}(\\textbf{x})}{\\hat{\\sigma}_{gi}(\\textbf{x})}\\right)$$\n",
    "Where $\\lambda_i$ is the upper bound of constraint $g_i$, $\\hat{\\mu}_{gi}$ and $\\hat{\\sigma}_{gi}$ are respectively the predictive mean and variance of constraint $g_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_i = 40\n",
    "y_min = np.min(y_t[np.where(g_t <= lambda_i)[0]])\n",
    "\n",
    "\n",
    "def ExpectedImprovement(x, objective, y_min):\n",
    "    from scipy.special import erf\n",
    "\n",
    "    mu = objective.predict_values(x)\n",
    "    var = objective.predict_variances(x) + 1e-6\n",
    "    EI = (y_min - mu) * (0.5 + 0.5 * erf((y_min - mu) / np.sqrt(var * 2))) + np.sqrt(\n",
    "        var / (2 * np.pi)\n",
    "    ) * np.exp(-pow(y_min - mu, 2) / (2 * var))\n",
    "    return EI\n",
    "\n",
    "\n",
    "def ProbabilityOfFeasibility(x, constraint, lambda_i):\n",
    "    from scipy.special import erf\n",
    "\n",
    "    mu = constraint.predict_values(x)\n",
    "    var = constraint.predict_variances(x) + 1e-6\n",
    "    PF = 0.5 + 0.5 * erf((lambda_i - mu) / np.sqrt(var * 2))\n",
    "    return PF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the optimal variabel combination $\\textbf{x}^*$\n",
    "The purpose is to find the optimal variabel combination $\\textbf{x}^*$, which can be found by maximizing the aquisition function $\\text{cEI(\\textbf{x})}$:\n",
    "$$ \\textbf{x}^* = \\underset{\\textbf{x} \\in \\textbf{X}}{\\text{arg max }} \\text{cEI}(\\textbf{x})$$\n",
    "For the maximization purpose Evolutionary Strategy of the  python package Pymoo is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.algorithms.soo.nonconvex.es import ES\n",
    "from pymoo.optimize import minimize\n",
    "import numpy as np\n",
    "from pymoo.core.problem import Problem\n",
    "\n",
    "\n",
    "def cEI_problem(x):\n",
    "    return ExpectedImprovement(x, y_hat, y_min) * ProbabilityOfFeasibility(\n",
    "        x, g_hat, lambda_i\n",
    "    )\n",
    "\n",
    "\n",
    "class MyProblem(Problem):\n",
    "    def __init__(self, object, constraint, y_min, lambda_i):\n",
    "        self.object = object\n",
    "        self.constraint = constraint\n",
    "        self.y_min = y_min\n",
    "        self.lambda_i = lambda_i\n",
    "        super().__init__(n_var=2, n_obj=1, xl=np.array([-5, 0]), xu=np.array([10, 15]))\n",
    "\n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        cEI = ExpectedImprovement(\n",
    "            x, self.object, self.y_min\n",
    "        ) * ProbabilityOfFeasibility(x, self.constraint, self.lambda_i)\n",
    "        out[\"F\"] = -cEI\n",
    "\n",
    "\n",
    "problem = MyProblem(y_hat, g_hat, y_min, lambda_i)\n",
    "algorithm = ES(n_offsprings=200, rule=1.0 / 7.0)\n",
    "\n",
    "res = minimize(problem, algorithm, (\"n_gen\", 200), seed=1, verbose=False)\n",
    "\n",
    "clear_output()\n",
    "print(\"Best solution found: \\nX = %s\\nF = %s\" % (res.X, res.F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating plots to illustrate the components of cEI ($\\textbf{x}^*$ is shown as yellow star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Create plotting data\n",
    "EI_M = np.zeros([num, num])\n",
    "PF_M = np.zeros([num, num])\n",
    "cEI_M = np.zeros([num, num])\n",
    "for i in range(num):\n",
    "    EI = ExpectedImprovement(np.array([X[:, i], Y[:, i]]).T, y_hat, y_min)\n",
    "    PF = ProbabilityOfFeasibility(np.array([X[:, i], Y[:, i]]).T, g_hat, lambda_i)\n",
    "    EI_M[:, i] = (\n",
    "        EI.flatten() + 1e-12\n",
    "    )  # The 1e-12 is only added to ensure a pretty plot and does not affect the result\n",
    "    PF_M[:, i] = PF.flatten()\n",
    "    cEI_M[:, i] = (\n",
    "        (EI * PF).flatten() + 1e-12\n",
    "    )  # The 1e-12 is only added to ensure a pretty plot and does not affect the result\n",
    "\n",
    "\n",
    "n_levels = 20\n",
    "clear_output()\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# Ploting objective function\n",
    "contour1 = axes[0].contourf(\n",
    "    X, Y, EI_M, levels=n_levels, cmap=\"RdBu_r\"\n",
    ")  # Filled contours in red and blue\n",
    "contour_lines = axes[0].contour(\n",
    "    X, Y, EI_M, levels=n_levels, colors=\"black\"\n",
    ")  # Contour lines in black\n",
    "fig.colorbar(contour1, ax=axes[0])\n",
    "axes[0].set_title(\"Expected Improvement\", fontsize=16)\n",
    "axes[0].set_xlabel(\"x1\")\n",
    "axes[0].set_ylabel(\"x2\")\n",
    "\n",
    "# Ploting constraint function\n",
    "contour2 = axes[1].contourf(\n",
    "    X, Y, PF_M, levels=n_levels, cmap=\"RdBu_r\"\n",
    ")  # Filled contours in red and blue\n",
    "contour_lines = axes[1].contour(\n",
    "    X, Y, PF_M, levels=n_levels, colors=\"black\"\n",
    ")  # Contour lines in black\n",
    "fig.colorbar(contour2, ax=axes[1])\n",
    "axes[1].set_title(\"Probability of Feasibility\", fontsize=16)\n",
    "axes[1].set_xlabel(\"x1\")\n",
    "axes[1].set_ylabel(\"x2\")\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(6, 4))\n",
    "# Ploting objective function\n",
    "contour1 = axes.contourf(\n",
    "    X, Y, cEI_M, levels=n_levels, cmap=\"RdBu_r\"\n",
    ")  # Filled contours in red and blue\n",
    "contour_lines = axes.contour(\n",
    "    X, Y, cEI_M, levels=n_levels, colors=\"black\"\n",
    ")  # Contour lines in black\n",
    "axes.scatter([res.X[0]], [res.X[1]], marker=\"*\", color=\"yellow\", s=50)\n",
    "fig.colorbar(contour1, ax=axes)\n",
    "axes.set_title(\"constrained Expected Improvement\", fontsize=16)\n",
    "axes.set_xlabel(\"x1\")\n",
    "axes.set_ylabel(\"x2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
