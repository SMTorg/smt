{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"https://colab.research.google.com/github/SMTorg/smt/blob/master/tutorial/SMT_PODI_Airfoil.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"jumbotron text-left\"><b>\n",
    "\n",
    "This tutorial describes how to use PODI, an application of the SMT toolbox. \n",
    "It combines Proper Orthogonal Decomposition (POD) and kriging based surrogate models to perform the estimations of vectorial outputs.\n",
    "    \n",
    "PODI is applied to predict a Transonic Airfoil Pressure\n",
    "\n",
    "</div>\n",
    "\n",
    "Hugo REIMERINGER, Nathalie BARTOLI, Sylvain DUBREUIL, InÃªs CARDOSO, Paul SAVES  ONERA/DTIS/M2CI\n",
    "    \n",
    "Giovanni CATALANI,Joseph MORLIER ISAE-SUPAERO\n",
    "    \n",
    " \n",
    "September  2024 - `SMT version 2.6.3`\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"alert alert-success\" style=\"padding:1em\">\n",
    "To use SMT, please follow this link : https://github.com/SMTorg/SMT/blob/master/README.md. The documentation is available here: http://smt.readthedocs.io/en/latest/\n",
    "</p>\n",
    "\n",
    "The reference paper is available\n",
    "here https://www.sciencedirect.com/science/article/pii/S0965997818309360?via%3Dihub\n",
    "\n",
    "or as a preprint: http://mdolab.engin.umich.edu/content/python-surrogate-modeling-framework-derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"alert alert-warning\" style=\"padding:1em\">\n",
    "To use PODI function from SMT, please refer to the dedicated notebook: SMT_PODI_tutorial.ipynb\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transonic Airfoil Pressure Prediction with Kriging Interpolation and POD = PODI\n",
    "\n",
    "In this tutorial, we demonstrate how to analyze airfoil pressure data using Principal Orthogonal Decomposition (POD) and surrogate modeling with Kriging (KRG), leveraging the Gaussian process framework. The goal is to predict pressure distributions on a transonic airfoil under various flight conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from smt.applications import PODI\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as tri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Processing for Transonic Airfoil Analysis\n",
    "\n",
    "Data can be loaded at:\n",
    "https://zenodo.org/records/12700680?token=eyJhbGciOiJIUzUxMiJ9.eyJpZCI6IjQyNzI4M2NmLWIwYjktNDc1Ny1hYjA5LTliYjU4YjY4MjFmNCIsImRhdGEiOnt9LCJyYW5kb20iOiI5ZjY5MWIzNWQ5MTRmNGE4ZDdjNmY4ZjI4MTY1NDAyMiJ9._BqW0JKCMiI89PjbTmNOtbvYO6iCBx-hjP4WRPGepV2ufmAlqk_SEmAgbPfqkW9YvjOsh67lHn2jGQ7cg_n1nw\n",
    "\n",
    "1. **Load the Dataset**: The `TransonicRAE` class is instantiated with paths to the data directory and the specific target field ('Pressure') for analysis.\n",
    "\n",
    "2. **Normalize the Data**:\n",
    "   - **Conditions**: The angle of attack (`Alpha`) and freestream velocity (`Vinf`). As SMT normalized the input data, no transformation is done here\n",
    "   - **Target Field (Pressure)**: Pressure data is normalized using the mean and standard deviation for model stability and efficient training.\n",
    "3. **Split the Data**: The dataset is split into training and test sets based on specified ratios to ensure robust model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "data_directory = \"data/\"\n",
    "save_directory = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset...\n",
      "Loading raw data\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/db_random.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2349520/2353229829.py\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Create dataset instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransonicRAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Pressure\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0mcoef_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2349520/2353229829.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_directory, target_field)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing dataset...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2349520/2353229829.py\u001b[0m in \u001b[0;36mprocess_data\u001b[0;34m(self, data_directory, target_field)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading raw data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         db_random = np.load(\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"db_random.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         ).item()\n",
      "\u001b[0;32m~/miniconda3/envs/newenv1/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/db_random.npy'"
     ]
    }
   ],
   "source": [
    "class TransonicRAE:\n",
    "    def __init__(self, data_directory, target_field):\n",
    "        print(\"Processing dataset...\")\n",
    "        self.dataset, self.coef_norm = self.process_data(data_directory, target_field)\n",
    "\n",
    "    def process_data(self, data_directory, target_field):\n",
    "        print(\"Loading raw data\")\n",
    "        db_random = np.load(\n",
    "            os.path.join(data_directory, \"db_random.npy\"), allow_pickle=True\n",
    "        ).item()\n",
    "        db_cyc = np.load(\n",
    "            os.path.join(data_directory, \"db_cyc.npy\"), allow_pickle=True\n",
    "        ).item()\n",
    "\n",
    "        # Merge db_random and db_cyc\n",
    "        db = {\n",
    "            key: np.concatenate((db_random[key], db_cyc[key]), axis=0)\n",
    "            for key in [\n",
    "                \"Pressure\",\n",
    "                \"Xcoordinate\",\n",
    "                \"Ycoordinate\",\n",
    "                \"Vinf\",\n",
    "                \"Alpha\",\n",
    "                \"idx\",\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        print(\"Raw data loaded, applying filters and normalizing pressure data\")\n",
    "\n",
    "        # Filter the entries where Vinf is at least 20% of 347 m/s\n",
    "        velocity_threshold = 0.2 * 347\n",
    "        valid_indices = db[\"Vinf\"] >= velocity_threshold\n",
    "\n",
    "        conditions = np.column_stack(\n",
    "            (db[\"Alpha\"][valid_indices], db[\"Vinf\"][valid_indices])\n",
    "        )\n",
    "\n",
    "        self.X_coord = (\n",
    "            2\n",
    "            * (db[\"Xcoordinate\"][0] - db[\"Xcoordinate\"][0].min())\n",
    "            / (db[\"Xcoordinate\"][0].max() - db[\"Xcoordinate\"][0].min())\n",
    "            - 1\n",
    "        )\n",
    "        self.Y_coord = (\n",
    "            2\n",
    "            * (db[\"Ycoordinate\"][0] - db[\"Ycoordinate\"][0].min())\n",
    "            / (db[\"Ycoordinate\"][0].max() - db[\"Ycoordinate\"][0].min())\n",
    "            - 1\n",
    "        )\n",
    "\n",
    "        # Normalize target field\n",
    "        mean_out = db[target_field].mean()\n",
    "        std_out = db[target_field].std()\n",
    "        normalized_field = (db[target_field][valid_indices] - mean_out) / std_out\n",
    "\n",
    "        coef_norm = {\"mean\": mean_out, \"std\": std_out}\n",
    "\n",
    "        return (conditions, normalized_field), coef_norm\n",
    "\n",
    "    def create_splits(self, train_ratio=0.9, test_ratio=0.1, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        num_samples = self.dataset[0].shape[0]\n",
    "        indices = np.random.permutation(num_samples)\n",
    "\n",
    "        train_end = int(train_ratio * num_samples)\n",
    "\n",
    "        train_indices = indices[:train_end]\n",
    "        test_indices = indices[train_end:]\n",
    "\n",
    "        train_data = (self.dataset[0][train_indices], self.dataset[1][train_indices])\n",
    "        test_data = (self.dataset[0][test_indices], self.dataset[1][test_indices])\n",
    "\n",
    "        return train_data, test_data\n",
    "\n",
    "\n",
    "# Create dataset instance\n",
    "dataset = TransonicRAE(data_directory, \"Pressure\")\n",
    "train_data, test_data = dataset.create_splits()\n",
    "coef_norm = dataset.coef_norm\n",
    "print(\"Number of training points\", np.shape(train_data[0])[0])\n",
    "print(\"Number of testing points\", np.shape(test_data[0])[0])\n",
    "\n",
    "X = dataset.X_coord\n",
    "Y = dataset.Y_coord\n",
    "print(\"Shape dataset for pressure (number of mesh points)\", np.shape(X), np.shape(Y))\n",
    "# Load Airfoil\n",
    "airfoil = np.load(os.path.join(data_directory, \"airfoil.npy\"))\n",
    "X_airfoil = airfoil[:, 0] - 0.5\n",
    "Y_airfoil = airfoil[:, 1]\n",
    "print(\n",
    "    \"Shape data for airfoil (Coordinates of the airfoil boundary.)\",\n",
    "    np.shape(X_airfoil),\n",
    "    np.shape(Y_airfoil),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshot Matrix Classes\n",
    "\n",
    "The `SnapMatrix` class is designed to handle snapshot matrices of data, typically used as input for the POD method. It includes:\n",
    "\n",
    "- **assemble**: Constructs the snapshot matrix from a list of data snapshots. Each snapshot is flattened and stored as a row in the matrix, enabling efficient manipulation for POD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnapMatrix:\n",
    "    def __init__(self, labels):\n",
    "        \"\"\"\n",
    "        Initialize the Snapshot Matrix\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        labels:  data snapshots\n",
    "        \"\"\"\n",
    "        self.labels = labels\n",
    "\n",
    "    def assemble(self):\n",
    "        \"\"\"\n",
    "        Assemble the snapshot matrix\n",
    "\n",
    "        \"\"\"\n",
    "        # Snapshot Matrix\n",
    "        U_shape = np.zeros(self.labels[0].shape)\n",
    "        U_tilde = np.zeros((len(self.labels), len(U_shape.flatten())))\n",
    "\n",
    "        for i in range(0, len(self.labels)):\n",
    "            U_tilde[i, :] = self.labels[i].flatten()\n",
    "\n",
    "        return U_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Snapshot Matrix from the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pressure values and conditions from the training and test dataset\n",
    "P_train = train_data[1]\n",
    "P_test = test_data[1]\n",
    "\n",
    "train_x = train_data[0]\n",
    "test_x = test_data[0]\n",
    "\n",
    "\n",
    "# Calculate the mean across the pressure samples and remove it\n",
    "P_mean = np.mean(P_train, axis=0)\n",
    "P_train = P_train - P_mean\n",
    "# Assemble Snapshot Matrix\n",
    "U = SnapMatrix(P_train).assemble()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To use PODI from SMT for POD + INTERPOLATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podi = PODI()\n",
    "snapshot = U.T\n",
    "print(\"Dimension of the snapshot matrix:\", np.shape(snapshot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To compute PODI we can give the number of modes or give a threshold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PODI computation with a fixed number of modes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 10\n",
    "podi.compute_pod(database=snapshot, n_modes=latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PODI computation with a threshold value to choose the number of modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_pod = 42\n",
    "threshold = 0.97\n",
    "podi.compute_pod(database=snapshot, tol=threshold, seed=seed_pod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results of the POD + SVD\n",
    "n_modes = podi.get_n_modes()\n",
    "print(f\"{n_modes} modes were kept.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the kriging based models (default is Kriging \"KRG\")\n",
    "podi.set_interp_options(\n",
    "    interp_type=\"KRG\",\n",
    "    interp_options=[{\"corr\": \"matern52\", \"nugget\": 1e-10, \"hyper_opt\": \"Cobyla\"}],\n",
    ")\n",
    "# Setting the training values: SMT is doing the data normalization\n",
    "podi.set_training_values(xt=train_x)\n",
    "# Training the models\n",
    "podi.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the desired values with inputs\n",
    "# SMT is doing the data normalization\n",
    "xv = test_x\n",
    "values = podi.predict_values(xv)\n",
    "variances = podi.predict_variances(xv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Possibility to compute some error metrics (costly to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [interp_error, proj_error, tot_error] = podi.compute_pod_errors(xt = train_x, database = snapshot)\n",
    "# , interp_type = \"KRG\", interp_options=[{'corr' : 'matern52', 'nugget': 1e-10,}])\n",
    "# print(\"interpolation error\", interp_error)\n",
    "# print(\"projection error\", proj_error)\n",
    "# print(\"total error\", tot_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Predictions vs. Ground Truth for Airfoil Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_vs_truth(\n",
    "    X_airfoil, Y_airfoil, X, Y, P_pred_list, P_test_list, num_samples=5, seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot predicted and actual pressure distributions for random samples.\n",
    "\n",
    "    Parameters:\n",
    "    - X_airfoil, Y_airfoil: Coordinates of the airfoil boundary.\n",
    "    - X, Y: Coordinates for pressure values.\n",
    "    - P_pred_list, P_test_list: Lists of predicted and actual pressure matrices.\n",
    "    - num_samples: Number of random samples to plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate random indices to select random samples\n",
    "    # Set the seed\n",
    "    np.random.seed(seed)\n",
    "    sample_indices = np.random.choice(len(P_pred_list), num_samples, replace=False)\n",
    "\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        # Setting up the plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "        plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "        # Triangulation for contour plotting\n",
    "        triang = tri.Triangulation(X, Y)\n",
    "\n",
    "        # Plot ground truth\n",
    "        ax = axes[0]\n",
    "        contour_gt = ax.tricontourf(\n",
    "            triang, P_test_list[idx].flatten(), levels=100, cmap=\"viridis\"\n",
    "        )\n",
    "        ax.set_title(f\"Ground Truth Sample {idx+1}\")\n",
    "        ax.set_xlabel(\"X Coordinate\")\n",
    "        ax.set_ylabel(\"Y Coordinate\")\n",
    "        fig.colorbar(contour_gt, ax=ax, orientation=\"vertical\")\n",
    "\n",
    "        # Plot prediction\n",
    "        ax = axes[1]\n",
    "        contour_pred = ax.tricontourf(\n",
    "            triang, P_pred_list[idx].flatten(), levels=100, cmap=\"viridis\"\n",
    "        )\n",
    "        ax.set_title(f\"Predicted PODI Pressure Sample {idx+1}\")\n",
    "        ax.set_xlabel(\"X Coordinate\")\n",
    "        fig.colorbar(contour_pred, ax=ax, orientation=\"vertical\")\n",
    "\n",
    "        # Overlay the airfoil area in white on both plots\n",
    "        for ax in axes:\n",
    "            ax.fill(X_airfoil, Y_airfoil, \"white\")\n",
    "            ax.set_aspect(\"equal\", \"box\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add the mean values and come back to the initial pressure values\n",
    "P_pred_podi = P_mean.flatten() + values.T\n",
    "P_pred_podi = P_pred_podi * dataset.coef_norm[\"std\"] + dataset.coef_norm[\"mean\"]\n",
    "P_test = [(p * dataset.coef_norm[\"std\"] + dataset.coef_norm[\"mean\"]) for p in P_test]\n",
    "\n",
    "# Convert P_pred to a list of arrays, each with shape (num_points, 1) for consistency with P_test\n",
    "P_pred_list_podi = [\n",
    "    P_pred_podi[i, :].reshape(-1, 1) for i in range(P_pred_podi.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Number of points chosen randomly to compare the pressure\n",
    "num_samples = 10\n",
    "plot_predictions_vs_truth(\n",
    "    X_airfoil,\n",
    "    Y_airfoil,\n",
    "    X,\n",
    "    Y,\n",
    "    P_pred_list_podi,\n",
    "    P_test,\n",
    "    num_samples=num_samples,\n",
    "    seed=seed_pod,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
